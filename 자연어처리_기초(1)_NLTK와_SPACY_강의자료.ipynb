{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OOJNIM/digitalhumanities/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EA%B8%B0%EC%B4%88(1)_NLTK%EC%99%80_SPACY_%EA%B0%95%EC%9D%98%EC%9E%90%EB%A3%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia1YizjfuABU"
      },
      "source": [
        "# 자연어처리 기초(1) NLTK와 SPACY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmWaKNyouIME"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYSWeEaWvQ0N"
      },
      "source": [
        "### NLTK 알아보기\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "NLTK(Natural Language Toolkit)는 언어 처리 기능을 제공하는 파이썬 라이브러리. 손쉽게 이용할\n",
        "수 있도록 모듈 형식으로 제공하고 있음. 이러한 모듈의 범주로 분류 토큰화(tokenization), 스테밍(stemming)와 같은 언어 전처리 모듈 부터 구문분석(parsing)과 같은 언어 분석, 클러스터링, 감정 분석(sentiment analysis), 태깅(tagging)과 같은 분석 모듈 시맨틱 추론(semantic reasoning)과 같이 보다 고차원 적인 추론 모듈도 제공하고 있음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F-lXCuxzmQX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "dbdcc94a-d63d-42c0-a233-6ddb359b6120"
      },
      "source": [
        "nltk.download()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6e230a00a763>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m                 \u001b[0mDownloaderGUI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTclError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m                 \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"d\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_interactive_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"u\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_interactive_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_simple_interactive_download\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  Identifier> \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"l\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m                     self._ds.list(\n\u001b[0m\u001b[1;32m   1185\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                         \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mlist\u001b[0;34m(self, download_dir, show_packages, show_collections, header, more_prompt, skip_installed)\u001b[0m\n\u001b[1;32m    567\u001b[0m                 \u001b[0mlines\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# for more_prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmore_prompt\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m                     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hit Enter to continue: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"q\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvoegCvCyAl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a19aed-bff7-42b8-ced5-6f83a2b80604"
      },
      "source": [
        "# import all the resources for Natural Language Processing with Python\n",
        "nltk.download(\"book\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK62J0rM2ITL"
      },
      "source": [
        "### 토큰화(Tokenizing)란?\n",
        "\n",
        "---\n",
        "\n",
        "자연어 처리에서 크롤링 등으로 얻어낸 코퍼스 데이터가 필요에 맞게 전처리되지 않은 상태라면, 해당 데이터를 사용하고자하는 용도에 맞게 토큰화(tokenization) & 정제(cleaning) & 정규화(normalization)하는 일을 하게 함.\n",
        "\n",
        "주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 부름. 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의. 크게 \"단어 토큰화 (Word Tokenizing)\"과 \"문장 토큰화 (Sentence Tokenizing)\" 등이 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJnVK-bUzHUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed4979c4-c4f2-4045-8576-3e80eeb85fa9"
      },
      "source": [
        "sentence = \"NLTK is a leading platform for building Python programs to work with human language data\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "print(tokens)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9StKC9f12mwT"
      },
      "source": [
        "### 품사태깅, POS (Part of Speech) tagging란?\n",
        "\n",
        "---\n",
        "Part-Of-Speech tagging(POS tagging)은 문장 내 단어들의 품사를 식별하여 태그를 붙여주는 것을 말함. 투플(tuple)의 형태로 출력되며 (단어, 태그)로 출력됨. 여기서 태그는 품사(POS) 태그임\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gevPsvzZzRMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598b5357-aa92-4ca1-f680-82816321025c"
      },
      "source": [
        "tagged = nltk.pos_tag(tokens)\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2iNVLgMzVIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f75e1b26-0236-4d45-809d-591a01d8ce5a"
      },
      "source": [
        "entities = nltk.chunk.ne_chunk(tagged)\n",
        "print(entities)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (ORGANIZATION NLTK/NNP)\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  leading/VBG\n",
            "  platform/NN\n",
            "  for/IN\n",
            "  building/VBG\n",
            "  (PERSON Python/NNP)\n",
            "  programs/NNS\n",
            "  to/TO\n",
            "  work/VB\n",
            "  with/IN\n",
            "  human/JJ\n",
            "  language/NN\n",
            "  data/NNS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxFQnEr04boo"
      },
      "source": [
        "## Part 2. Spacy\n",
        "\n",
        "---\n",
        "\n",
        "스스로 산업을 위한 강력한 NLP라이브러리라고 설명하고 있으며 NLTK와 마찬가지로 언어처리를 위해 만들어진 라이브러리.\n",
        "\n",
        "[Spacy document](https://spacy.io/)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1. 언어모델 설치**\n",
        "\n",
        "Spacy는 영어 뿐만 아니라 프랑스어, 독일어, 스페인어, 터키어, 한국어, 일본어 등 다양한 언어모델을 제공하고 있음."
      ],
      "metadata": {
        "id": "tLjzv2hOvvLj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcuaWQcn4c0o"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#언어 모델 설치 - 영어\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "QOi1H_QQvRHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3166b3e2-9a21-4dcb-d0eb-3e5e7f4c98ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-31 00:37:01.027790: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-31 00:37:01.027864: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-31 00:37:01.027922: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-31 00:37:02.458642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#언어 모델 설치 - 프랑스어\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "id": "E-fcJAHJvImK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237c547a-c1fd-4515-8596-dad99dc96fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-31 00:37:49.380641: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-31 00:37:49.380723: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-31 00:37:49.380762: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-31 00:37:50.734518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting fr-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.6.0/fr_core_news_sm-3.6.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 언어 모델 설치 - 독일어\n"
      ],
      "metadata": {
        "id": "AWp-_Sc0u5zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an2QWsGN5LUQ"
      },
      "source": [
        "# 언어 모델 설치 - 스페인어\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPACY를 활용한 토큰화와 품사태깅**"
      ],
      "metadata": {
        "id": "E2jMnL-0vghE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#영어\n",
        "import en_core_web_sm\n",
        "nip = en_core_web_sm.load()\n",
        "doc = nip(u\"My name is Peter and his name is Jack\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_)\n"
      ],
      "metadata": {
        "id": "PuosOf9Tv6df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f88bf63-ba30-4db5-f502-6ef38070234d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My PRON\n",
            "name NOUN\n",
            "is AUX\n",
            "Peter PROPN\n",
            "and CCONJ\n",
            "his PRON\n",
            "name NOUN\n",
            "is AUX\n",
            "Jack PROPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#프랑스어\n",
        "import fr_core_news_sm\n",
        "nip_2 = fr_core_news_sm.load()\n",
        "doc_2 = nip_2(u'Bonjour je m\\'appelle minjoo')\n",
        "\n",
        "for token in doc_2:\n",
        "  print(token.text, token.pos_)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqnR3HgKwGS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b135aa-564c-41a4-b742-f23964514ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonjour PROPN\n",
            "je PRON\n",
            "m' PRON\n",
            "appelle VERB\n",
            "minjoo ADJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#독일어\n",
        "\n"
      ],
      "metadata": {
        "id": "mlpKenr1yPl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9iSNuhz7IKo"
      },
      "source": [
        "#스페인어\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPACY를 활용한 부분 구문 분석**"
      ],
      "metadata": {
        "id": "8MzyVWC2Cq99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#영어\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "doc = nlp(u\"Mary slapped the green witch\")\n",
        "\n",
        "for  chunk in  doc.noun_chunks:\n",
        "  print(chunk.text, chunk.label_, chunk.root.text, chunk.root.head.text)"
      ],
      "metadata": {
        "id": "VUwKA6UUCsNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ebcec6e-bd65-4ca4-9434-48722072fe34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mary NP Mary slapped\n",
            "the green witch NP witch slapped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#프랑스어\n",
        "import fr_core_news_sm\n",
        "nlp2 = fr_core_news_sm.load()\n",
        "doc2 = nlp2(u\"Dans un environnement épanouissant alliant multilinguisme et pluriculturalité, le LFS accueille des élèves de toutes nationalités et leur offre une éducation française et internationale stimulante, les préparant au Baccalauréat et à l’enseignement supérieur en France et dans le monde entier.\")\n",
        "\n",
        "for  chunk1 in doc2.noun_chunks:\n",
        "  # print(chunk1.text, chunk1.label_, chunk1.root.text, chunk1.root.head.text)\n",
        "  print('text=', chunk1.text, 'label=',chunk1.label_,'root_text=', chunk1.root.text,'root_head_text=',chunk1.root.head.text )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f5F-vyGACsUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2bfd5f1-4132-4d01-c251-8bdb2b09171d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text= un environnement label= NP root_text= environnement root_head_text= environnement\n",
            "text= multilinguisme label= NP root_text= multilinguisme root_head_text= alliant\n",
            "text= pluriculturalité label= NP root_text= pluriculturalité root_head_text= multilinguisme\n",
            "text= le LFS label= NP root_text= LFS root_head_text= environnement\n",
            "text= élèves label= NP root_text= élèves root_head_text= accueille\n",
            "text= toutes nationalités label= NP root_text= nationalités root_head_text= élèves\n",
            "text= leur offre label= NP root_text= offre root_head_text= environnement\n",
            "text= une éducation française et internationale label= NP root_text= éducation root_head_text= offre\n",
            "text= Baccalauréat label= NP root_text= Baccalauréat root_head_text= préparant\n",
            "text= à l’ label= NP root_text= l’ root_head_text= Baccalauréat\n",
            "text= France label= NP root_text= France root_head_text= supérieur\n",
            "text= dans le monde label= NP root_text= monde root_head_text= environnement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#독일어\n"
      ],
      "metadata": {
        "id": "LLnmQOwmCsZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#스페인어\n"
      ],
      "metadata": {
        "id": "bBiJfI5jCses"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYqTpXeL72ZE"
      },
      "source": [
        "### 개체명 인식 (Name Entity Recognition)이란?\n",
        "\n",
        "---\n",
        "\n",
        "개체명은 사람, 국가 ,제품, 조직과 같이 이름이 지정된 현실 속의 객체를 말함. spacy는 아래와 같은 개체 유형을 기본으로 제공.\n",
        "\n",
        "\n",
        "\n",
        "1.   PERSON: 실제와 허구를 포함한 사람\n",
        "2.   NORP: 민족, 종교 또는 정치 그룹\n",
        "3. FACILITY: 건물, 공항, 고속도로, 교량과 같은 시설\n",
        "4. ORG: 기업, 단체, 기관과 같은 조직\n",
        "5. GPE: 국가, 도시, 주 등과 같은 지역명\n",
        "6. LOC: GPE 이외의 산맥, 수역 등과 같은 지역의 유형\n",
        "7. PRODUCT: 객체, 차량, 식품 등과 같은 서비스를 제외한 제품\n",
        "8. EVENT: 이름이 있는 허리케인, 전투, 전쟁, 스포츠 행사와 같은 이벤트\n",
        "9. WORK_OF_ART: 책, 노래 등과 같은 예술의 제목\n",
        "10. LAW: 법에 의해 명명된 문서\n",
        "11. LANGUAGE: 이름 지어진 모든 언어\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#영어\n",
        "\n",
        "doc3 = nlp(u'there are many offices of Microsoft in New York')\n",
        "for ent in doc3.ents:\n",
        "  print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "\n",
        "#개체명은 Doc 객체의 ent 속성에서 확인할 수 있음.\n"
      ],
      "metadata": {
        "id": "Y2iA_NzJynZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424bd969-3733-43d5-cf02-253a97c5f621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Microsoft 26 35 ORG\n",
            "New York 39 47 GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#프랑스어\n",
        "\n",
        "for ent1 in doc2.ents:\n",
        "  print(ent1.text, ent1.start_char, ent1.end_char, ent1.label_)\n",
        "\n",
        "\n",
        "#개체명은 Doc 객체의 ent 속성에서 확인할 수 있음.\n"
      ],
      "metadata": {
        "id": "HDZnfbKuynfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e6e25bd-5cdb-487b-df36-01b3a6f6cf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LFS 82 85 MISC\n",
            "Baccalauréat 215 227 LOC\n",
            "l’ 233 235 LOC\n",
            "France 261 267 LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#독일어\n",
        "\n",
        "\n",
        "\n",
        "#개체명은 Doc 객체의 ent 속성에서 확인할 수 있음.\n"
      ],
      "metadata": {
        "id": "a5YfoiqZynok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRygWmuE8lsx"
      },
      "source": [
        "#스페인어\n",
        "\n",
        "\n",
        "\n",
        "#개체명은 Doc 객체의 ent 속성에서 확인할 수 있음.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOTkOAP7-1MP"
      },
      "source": [
        "### 불용어 (stopwords)란?\n",
        "\n",
        "---\n",
        "데이터의 효과적인 분석을 위해, 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요. 여기서 \"큰 의미가 없다\"는 것은 분석을 하는데는 도움이 되지 않는 단어들. 데이터 처리 목적에 따라 \"큰 의미가 없는 단어\"들이 달라질 수 있지만 주로 대명사 (yo, tú, él, ella)나 ser, estar, hay 등의 동사등이 해당될수 있음.\n",
        "\n",
        "이러한 단어들을 불용어(stopwords)라고 하며 NLTK에서는 이러한 불용어를 패키지 내에서 미리 정의하여 제공. 또한 불용어는 데이터 처리 목적에 따라 개발자가 직접 정의할 수도 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**영어**"
      ],
      "metadata": {
        "id": "rsb4b3cf0CEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "print(stop_words)\n"
      ],
      "metadata": {
        "id": "ZaqS4dES0DW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1f33ca-a88a-46b3-df3c-1d3611c510f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \" The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or informally as America, is a country primarily located in North America and consisting of 50 states, a federal district, five major unincorporated territories, nine Minor Outlying Islands, and 326 Indian reservations. It is the world's third-largest country by both land and total area.\"\n",
        "words = [w for w in nltk.wordpunct_tokenize(data)]\n",
        "tokenized_doc = [item for item in words if item not in stop_words]\n",
        "print(tokenized_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0ZklaznTbbz",
        "outputId": "7c7f1040-7706-470d-c3a1-7a9a9fabbe1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'United', 'States', 'America', '(', 'U', '.', 'S', '.', 'A', '.', 'USA', '),', 'commonly', 'known', 'United', 'States', '(', 'U', '.', 'S', '.', 'US', ')', 'informally', 'America', ',', 'country', 'primarily', 'located', 'North', 'America', 'consisting', '50', 'states', ',', 'federal', 'district', ',', 'five', 'major', 'unincorporated', 'territories', ',', 'nine', 'Minor', 'Outlying', 'Islands', ',', '326', 'Indian', 'reservations', '.', 'It', 'world', \"'\", 'third', '-', 'largest', 'country', 'land', 'total', 'area', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_created = [',','.',';','(',')','),','\\u200b','-','\\'']\n",
        "tokenized_doc = [item for item in tokenized_doc if item not in stop_words_created]\n",
        "print(tokenized_doc)"
      ],
      "metadata": {
        "id": "wpgpjxt50HXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72619a3f-7914-4416-b88d-afabe20173fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'United', 'States', 'America', 'U', 'S', 'A', 'USA', 'commonly', 'known', 'United', 'States', 'U', 'S', 'US', 'informally', 'America', 'country', 'primarily', 'located', 'North', 'America', 'consisting', '50', 'states', 'federal', 'district', 'five', 'major', 'unincorporated', 'territories', 'nine', 'Minor', 'Outlying', 'Islands', '326', 'Indian', 'reservations', 'It', 'world', 'third', 'largest', 'country', 'land', 'total', 'area']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E3VFCRtJVUUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kOtjOpYi0bL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**프랑스어**"
      ],
      "metadata": {
        "id": "8eyNtl_10qvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('french')\n",
        "\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "zFkAJBma0tc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b3b358-417d-4168-b0af-6310c7d880e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = \"La France (Écouter), en forme longue depuis 1875 la République française (Écouter), est un État souverain transcontinental dont le territoire métropolitain s'étend en Europe de l'Ouest et dont le territoire ultramarin s'étend dans les océans Indien, Atlantique, Pacifique, ainsi qu'en Antarctique8 et en Amérique du Sud.\"\n",
        "\n",
        "words2 = [w for w in nltk.wordpunct_tokenize(data2)]\n",
        "tokenized_doc2 = [item for item in words2 if item not in stop_words]\n",
        "print(tokenized_doc2)"
      ],
      "metadata": {
        "id": "U-S414gY03qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f89654-a53f-435e-c29d-a55aba181293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['La', 'France', '(', 'Écouter', '),', 'forme', 'longue', 'depuis', '1875', 'République', 'française', '(', 'Écouter', '),', 'État', 'souverain', 'transcontinental', 'dont', 'territoire', 'métropolitain', \"'\", 'étend', 'Europe', \"'\", 'Ouest', 'dont', 'territoire', 'ultramarin', \"'\", 'étend', 'océans', 'Indien', ',', 'Atlantique', ',', 'Pacifique', ',', 'ainsi', \"'\", 'Antarctique8', 'Amérique', 'Sud', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nUgvHBXY1HYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**독일어**"
      ],
      "metadata": {
        "id": "M6mypcUi1Puo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vbMGsEVA1ULK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DsnZJSDP1hlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pOTdYigF1hww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**스페인어**"
      ],
      "metadata": {
        "id": "MWPGrt0Pz_M-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U60W8NEn9Zlg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT9t56Bq_i_R"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYDHk7h4AQKI"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J4-ECjxBHGm"
      },
      "source": [
        "### 표제어추출(Lemmatization)이란?\n",
        "\n",
        "---\n",
        "표제어(Lemma)는 한글로는 '표제어' 또는 '기본 사전형 단어'를 의미. 표제어 추출은 단어들로부터 표제어를 찾는 것을 의미. 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가 단어의 수를 줄여줘 데이터 처리를 더 쉽게 함. 예를 들어서 soy, eres, es는 서로 다른 스펠링이지만 그 뿌리 단어는 ser라고 보는 것이 그 예이며, 여기서 이 단어들의 표제어는 ser임"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#영어\n",
        "for token in nlp(data):\n",
        "  print(token.text, token.lemma_)\n"
      ],
      "metadata": {
        "id": "vBWYye112LZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb60ee8-de4e-4ae9-9244-7796a1e43467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   \n",
            "The the\n",
            "United United\n",
            "States States\n",
            "of of\n",
            "America America\n",
            "( (\n",
            "U.S.A. U.S.A.\n",
            "or or\n",
            "USA USA\n",
            ") )\n",
            ", ,\n",
            "commonly commonly\n",
            "known know\n",
            "as as\n",
            "the the\n",
            "United United\n",
            "States States\n",
            "( (\n",
            "U.S. U.S.\n",
            "or or\n",
            "US US\n",
            ") )\n",
            "or or\n",
            "informally informally\n",
            "as as\n",
            "America America\n",
            ", ,\n",
            "is be\n",
            "a a\n",
            "country country\n",
            "primarily primarily\n",
            "located locate\n",
            "in in\n",
            "North North\n",
            "America America\n",
            "and and\n",
            "consisting consist\n",
            "of of\n",
            "50 50\n",
            "states state\n",
            ", ,\n",
            "a a\n",
            "federal federal\n",
            "district district\n",
            ", ,\n",
            "five five\n",
            "major major\n",
            "unincorporated unincorporated\n",
            "territories territory\n",
            ", ,\n",
            "nine nine\n",
            "Minor Minor\n",
            "Outlying Outlying\n",
            "Islands Islands\n",
            ", ,\n",
            "and and\n",
            "326 326\n",
            "Indian indian\n",
            "reservations reservation\n",
            ". .\n",
            "It it\n",
            "is be\n",
            "the the\n",
            "world world\n",
            "'s 's\n",
            "third third\n",
            "- -\n",
            "largest large\n",
            "country country\n",
            "by by\n",
            "both both\n",
            "land land\n",
            "and and\n",
            "total total\n",
            "area area\n",
            ". .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#프랑스어\n",
        "for token1 in nlp(data2):\n",
        "  print(token1.text, token1.lemma_)\n"
      ],
      "metadata": {
        "id": "oJXlVb-w2LiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "145cebc4-b53d-42bd-e692-29ef7b0f568d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La La\n",
            "France France\n",
            "( (\n",
            "Écouter Écouter\n",
            ") )\n",
            ", ,\n",
            "en en\n",
            "forme forme\n",
            "longue longue\n",
            "depuis depuis\n",
            "1875 1875\n",
            "la la\n",
            "République République\n",
            "française française\n",
            "( (\n",
            "Écouter Écouter\n",
            ") )\n",
            ", ,\n",
            "est est\n",
            "un un\n",
            "État État\n",
            "souverain souverain\n",
            "transcontinental transcontinental\n",
            "do do\n",
            "nt not\n",
            "le le\n",
            "territoire territoire\n",
            "métropolitain métropolitain\n",
            "s'étend s'étend\n",
            "en en\n",
            "Europe Europe\n",
            "de de\n",
            "l'Ouest l'Ouest\n",
            "et et\n",
            "do do\n",
            "nt not\n",
            "le le\n",
            "territoire territoire\n",
            "ultramarin ultramarin\n",
            "s'étend s'étend\n",
            "dans dans\n",
            "les les\n",
            "océans océans\n",
            "Indien Indien\n",
            ", ,\n",
            "Atlantique Atlantique\n",
            ", ,\n",
            "Pacifique Pacifique\n",
            ", ,\n",
            "ainsi ainsi\n",
            "qu'en qu'en\n",
            "Antarctique8 Antarctique8\n",
            "et et\n",
            "en en\n",
            "Amérique Amérique\n",
            "du du\n",
            "Sud Sud\n",
            ". .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#독일어\n",
        "\n"
      ],
      "metadata": {
        "id": "iwzG5iXK2LpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic3O8hM6BHX6"
      },
      "source": [
        "#스페인어\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRd5n4x4DMjG"
      },
      "source": [
        "### **연습문제**\n",
        "\n",
        "1. 영어 혹은 자신의 전공어의 정의나 소개글을 1줄로 위키피디아에서 찾아 document 변수에 저장하시오.\n",
        "2. 위 1번의 문장을 토큰화 하시오.\n",
        "3. 2번의 결과에서 불용어를 제거하시오.\n",
        "4. 3번의 결과에서 표제어를 추출하시오.\n",
        "5. 4번의 결과에서 개체명 인식을 진행하시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnR9RZnvvCO-"
      },
      "source": [
        "#1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCHR3L0ID7GE"
      },
      "source": [
        "#2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3"
      ],
      "metadata": {
        "id": "tP72rg6B3xDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4"
      ],
      "metadata": {
        "id": "63VKHcQw3xL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5"
      ],
      "metadata": {
        "id": "NnNB-7PN3xUI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}